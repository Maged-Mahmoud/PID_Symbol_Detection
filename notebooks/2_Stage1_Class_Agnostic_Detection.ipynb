{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "598e5e6d-1563-4d87-8d30-65db69c760ba",
   "metadata": {},
   "source": [
    "### To-do:\n",
    "- [ ] Modify - full_pid_splits and select_crops to remove test crops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c34dba7-9fb8-4980-9a9c-6fb066eb1fc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:albumentations.check_version:A new version of Albumentations is available: 1.4.14 (you have 1.4.11). Upgrade using: pip install --upgrade albumentations\n"
     ]
    }
   ],
   "source": [
    "import albumentations as A\n",
    "from fastai.vision.all import *\n",
    "from natsort import natsorted\n",
    "import os, sys, gc, cv2, yaml\n",
    "from pathlib import Path\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "from utils.preprocess_utils import *\n",
    "from utils.stage1_utils import *\n",
    "from utils.yolo_utils import *\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa04f033-8cda-49f2-95f2-aef2f47f71e3",
   "metadata": {},
   "source": [
    "#### 1. Make train, val, test sets from full PIDs\n",
    "- **Ques:** Can you guess why full sheets are used to partition the datasets and not crops?\n",
    "- **Ans:** To prevent data leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5d68e29-e6a9-4de1-b5cb-fc65b8e9ced0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images: 10\n",
      "Train: 6, Val: 2, Test: 2\n",
      "train_ims: [Path('../data/sample_dataset/original/5.jpg'), Path('../data/sample_dataset/original/8.jpg'), Path('../data/sample_dataset/original/4.jpg'), Path('../data/sample_dataset/original/1.jpg'), Path('../data/sample_dataset/original/6.jpg'), Path('../data/sample_dataset/original/2.jpg')] \n",
      "val_ims: [Path('../data/sample_dataset/original/7.jpg'), Path('../data/sample_dataset/original/9.jpg')] \n",
      "test_ims:[Path('../data/sample_dataset/original/0.jpg'), Path('../data/sample_dataset/original/3.jpg')]\n"
     ]
    }
   ],
   "source": [
    "dataset_dir = Path('../data/sample_dataset/original/') # directory containing full PIDs (original)\n",
    "train_ims, val_ims, test_ims = make_train_val_test_split(dataset_dir, train_val_test_ratio=[0.64, 0.16, 0.2])\n",
    "#print(f'train_ims: {train_ims} \\nval_ims: {val_ims} \\ntest_ims:{test_ims}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7accd0-0117-445f-8902-4caa6ec5f5f3",
   "metadata": {},
   "source": [
    "#### 2. Make train, val, test sets by selecting crops from full PIDs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7df04172-bef3-4f4b-a1f9-a2dfe09aa8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of class_Aware crops\n",
    "crops_folder_pth = Path('../data/sample_dataset/patches_class_aware/')\n",
    "train_crops_aware, val_crops_aware, _ = select_crops(full_pid_splits = [train_ims, val_ims, _], crops_folder_pth=crops_folder_pth)\n",
    "\n",
    "# Get list of class_Agnostoc crops\n",
    "crops_folder_pth = Path('../data/sample_dataset/patches_class_agnostic/')\n",
    "train_crops_agnostic, val_crops_agnostic, _ = select_crops(full_pid_splits = [train_ims, val_ims, _], crops_folder_pth=crops_folder_pth)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53e31d7-1f26-46e5-a48f-295fa9ac40cf",
   "metadata": {},
   "source": [
    "#### 3. Make folders for training YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75f3ca2b-5847-45ab-bada-7ea822c5bf6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied 100 files to train folder\n",
      "Copied 24 files to val folder\n",
      "Copied 24 files to test folder\n"
     ]
    }
   ],
   "source": [
    "# for class Aware\n",
    "make_yolo_folders(dir_name='yolo_class_aware', \n",
    "                  train_images=train_crops_aware, \n",
    "                  val_images=val_crops_aware, \n",
    "                  test_images=test_crops_aware)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed828fee-79e7-413d-9920-0f73f7e4943f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied 100 files to train folder\n",
      "Copied 24 files to val folder\n",
      "Copied 24 files to test folder\n"
     ]
    }
   ],
   "source": [
    "# for class Agnostic\n",
    "make_yolo_folders(dir_name='yolo_class_agnostic', \n",
    "                  train_images=train_crops_agnostic, \n",
    "                  val_images=val_crops_agnostic, \n",
    "                  test_images=test_crops_agnostic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "622ce770-f88b-48f1-9826-cd1d374defae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for full pids (Not recommended) because A) Dataset is small and B) Images are of large size)\n",
    "to_run = False\n",
    "if to_run:\n",
    "    make_yolo_folders(dir_name='yolo_full', \n",
    "                      train_images=train_ims, \n",
    "                      val_images=val_ims, \n",
    "                      test_images=test_ims)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0add8425-7d96-4c02-820b-522c3b7a7926",
   "metadata": {},
   "source": [
    "#### 4. Train yolo models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8c3e75-c695-417a-8c2e-51b9dfd56776",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_yolo_model(yaml_filename = , epochs=2, patience=1, batch_size=8, imgsz=1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b73c25-137b-4f58-bf9d-93e386c8463f",
   "metadata": {},
   "source": [
    "#### 5. Use trained Yolo model for inferencing with SAHI workflow (for aggreagating results across large PIDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f62ff70-b850-48d8-b206-633c109d4542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results will be saved to: ..\\data\\sample_dataset/SAHI_results_agnostic\n",
      "SAHI not running\n"
     ]
    }
   ],
   "source": [
    "perform_SAHI(im_pths = test_ims, \n",
    "             weights_file='../models/sample_class_agnostic/best.pt', \n",
    "             slice_size=1024,\n",
    "             suffix = 'agnostic', \n",
    "             with_conf_score = True, \n",
    "             to_run = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80531ad2-bc09-475a-8058-15ded50ab7b5",
   "metadata": {},
   "source": [
    "#### Now, Stage-2 begins . . ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c4eec3-2754-48ca-870e-b296d147737f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab89ba96-193d-4e13-8b5c-fef8fa821ae1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
