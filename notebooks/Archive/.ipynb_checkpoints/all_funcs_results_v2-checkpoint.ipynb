{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e00cdbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "from fastai.vision.all import *\n",
    "from IPython.display import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "\n",
    "#############\n",
    "from BoundingBox import BoundingBox\n",
    "from BoundingBoxes import BoundingBoxes\n",
    "from Evaluator import *\n",
    "from utils import *\n",
    "##############\n",
    "from my_funcs import *\n",
    "    \n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('device:',device)\n",
    "clear_gpu_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3582a66b",
   "metadata": {},
   "source": [
    "1. modify plot_class_distribution with yolo txts\n",
    "\n",
    "#### Resource:\n",
    "1. Github-: Add citation\n",
    "https://github.com/rafaelpadilla/Object-Detection-Metrics/tree/master/samples/sample_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46832e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBoundingBoxes(GT_dir, pred_dir):\n",
    "    \"\"\"Read txt files containing bounding boxes (ground truth and detections).\"\"\"\n",
    "    \n",
    "    files = natsorted(get_files(GT_dir, extensions='.txt'))\n",
    "    allBoundingBoxes = BoundingBoxes()\n",
    "    \n",
    "    for f in files:\n",
    "        nameOfImage = f.name.replace(\".txt\", \"\") # this should be taken care of!!\n",
    "        f = str(f)\n",
    "        fh1 = open(f, \"r\")\n",
    "        for line in fh1:\n",
    "            line = line.replace(\"\\n\", \"\")\n",
    "            if line.replace(' ', '') == '':\n",
    "                continue\n",
    "            splitLine = line.split(\" \")\n",
    "            idClass = splitLine[0]  # class\n",
    "            x = float(splitLine[1])\n",
    "            y = float(splitLine[2])\n",
    "            x2 = float(splitLine[3])\n",
    "            y2 = float(splitLine[4])\n",
    "            bb = BoundingBox(nameOfImage,idClass, x, y, x2,y2, CoordinatesType.Absolute, (7168, 4561),BBType.GroundTruth,\n",
    "                format=BBFormat.XYX2Y2)\n",
    "            allBoundingBoxes.addBoundingBox(bb)\n",
    "        fh1.close()\n",
    "        \n",
    "    # Read detections\n",
    "    files = natsorted(get_files(pred_dir, extensions='.txt'))\n",
    "    for f in files:\n",
    "        \n",
    "        nameOfImage = f.name.replace(\".txt\", \"\")\n",
    "        f = str(f)\n",
    "        # Read detections from txt file\n",
    "        fh1 = open(f, \"r\")\n",
    "        for line in fh1:\n",
    "            line = line.replace(\"\\n\", \"\")\n",
    "            if line.replace(' ', '') == '':\n",
    "                continue\n",
    "            splitLine = line.split(\" \")\n",
    "            #print(splitLine)\n",
    "            idClass = splitLine[0]  # class\n",
    "            confidence = float(splitLine[1])  # confidence\n",
    "            x = float(splitLine[2])\n",
    "            y = float(splitLine[3])\n",
    "            x2 = float(splitLine[4])\n",
    "            y2 = float(splitLine[5])\n",
    "            bb = BoundingBox(nameOfImage,idClass,x,y,x2,y2,CoordinatesType.Absolute, (7168,4561),BBType.Detected,confidence,\n",
    "                format=BBFormat.XYX2Y2)\n",
    "            allBoundingBoxes.addBoundingBox(bb)\n",
    "        fh1.close()\n",
    "    return allBoundingBoxes\n",
    "\n",
    "\n",
    "def get_detection_metrics(GT_bboxes_xyxy, preds_bboxes_xyxy, dataset_dir, df_name):\n",
    "    ''' res_df_name = 'sampleds_awr' '''\n",
    "    \n",
    "    ##### Metrics ########\n",
    "\n",
    "    boundingboxes = getBoundingBoxes(GT_bboxes_xyxy, preds_bboxes_xyxy)\n",
    "    evaluator = Evaluator()\n",
    "\n",
    "    # metrics\n",
    "    metricsPerClass = evaluator.GetPascalVOCMetrics(\n",
    "        boundingboxes, IOUThreshold=0.5, method=MethodAveragePrecision.EveryPointInterpolation) \n",
    "\n",
    "    columns=['class', 'total_positives', 'TP', 'FP', 'recall', 'precision']\n",
    "\n",
    "    df = pd.DataFrame(columns = columns)\n",
    "\n",
    "    print(\"Average precision values per class:\\n\")\n",
    "    # Loop through classes to obtain their metrics\n",
    "    for mc in metricsPerClass:\n",
    "        c = mc['class']\n",
    "        precision = mc['precision']\n",
    "        recall = mc['recall']\n",
    "        average_precision = mc['AP']\n",
    "        total_p = mc['total positives']\n",
    "        TP = mc['total TP']\n",
    "        FP = mc['total FP']\n",
    "        # Print AP per class\n",
    "        if recall.shape[0]==0:\n",
    "            print(f'OOPS: class {c}') # no single entry\n",
    "            row = pd.DataFrame([[c, total_p, TP, FP, '-123', '-123']], columns=columns)\n",
    "        else:\n",
    "            print(f'Results: class {c}, Recall {recall[-1]}, Precision: {precision[-1]}, Average_precision {average_precision}')\n",
    "            row = pd.DataFrame([[c, total_p, TP, FP, recall[-1], precision[-1]]], columns=columns)\n",
    "\n",
    "        df = pd.concat([df, row], ignore_index=True)\n",
    "\n",
    "    df.to_csv(f'{dataset_dir}/{df_name}.csv', index=False)\n",
    "    return df\n",
    "\n",
    "def transfer_labels(src_dir, dest_dir, weights_file, num_classes):\n",
    "    \"\"\"src_dir: preds_agn, \n",
    "    dest_dir: preds_agn2awr\"\"\"\n",
    "\n",
    "    os.makedirs(dest_dir, exist_ok=True)\n",
    "\n",
    "    to_tensor = transforms.Compose([\n",
    "        transforms.Resize((224,224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) # helps in removing the affect of color\n",
    "    ])\n",
    "\n",
    "    model = torchvision.models.resnet18(pretrained=True)\n",
    "    model.fc = torch.nn.Linear(model.fc.in_features, num_classes)\n",
    "    model.load_state_dict(torch.load(weights_file))\n",
    "    model.eval()\n",
    "\n",
    "    txt_pths = natsorted(get_files(src_dir, extensions='.txt'))\n",
    "    for txt_pth in txt_pths:\n",
    "        name = txt_pth.name\n",
    "        im_pth = str(src_dir)+'/'+name.replace('.txt', '.jpg')\n",
    "        im = cv2.imread(str(im_pth))\n",
    "        H,W = im.shape[:2]\n",
    "        with open(txt_pth, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "            f.close()\n",
    "        ls = [list(map(float,l.split())) for l in lines] # each line as a list of floats\n",
    "\n",
    "\n",
    "        new_txt_pth = dest_dir+'/'+name\n",
    "        with open(f'{new_txt_pth}', 'w+') as txt_file:  \n",
    "            for l in ls:\n",
    "                class_id, score, xc,yc,w,h = l\n",
    "                xmin = (xc-w/2)*W\n",
    "                xmax = (xc+w/2)*W\n",
    "                ymin = (yc-h/2)*H\n",
    "                ymax = (yc+h/2)*H\n",
    "                cropped_im =crop_image(im, [xmin,ymin,xmax,ymax])\n",
    "\n",
    "                image_pil = Image.fromarray(cropped_im.astype('uint8'))  # Convert to PIL format\n",
    "                image_transformed = to_tensor(image_pil)\n",
    "                image_transformed = image_transformed.unsqueeze(0)  # Add a batch dimension at index 0\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    predictions = model(image_transformed)\n",
    "                    _, new_class = torch.max(predictions, 1) # tensor[20]\n",
    "                    new_class = new_class.item() # 20\n",
    "\n",
    "                    # update\n",
    "                    l[0] = new_class\n",
    "                    yolo_line = ' '.join(map(str, l)) + '\\n'\n",
    "                    txt_file.write(yolo_line)           \n",
    "        txt_file.close()\n",
    "    print('*** Transferring Labels done ***')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d571ba8b",
   "metadata": {},
   "source": [
    "# Inferencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "793956e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dict = {'Sundt': {\n",
    "    'dir': '../Datasets/Sundt/results/',\n",
    "    'classes-dir': '../Datasets/Sundt/classes/',\n",
    "    'yolo-awr-weights': 'runs/detect/sundt_class_aware/weights/best.pt' ,\n",
    "    'yolo-agn-weights': 'runs/detect/sundt_class_agnostic/weights/best.pt' ,\n",
    "    'one-shot-weights': '../Datasets/Sundt/One-shot/best_model_100_1_v1.pth',\n",
    "    'slice_sz': 608,\n",
    "    'num-classes':28},\n",
    "           \n",
    "    'asupid': {\n",
    "    'dir': '../Datasets/asupid/results/',\n",
    "    'classes-dir': '../Datasets/asupid/classes/',\n",
    "    'yolo-awr-weights': 'runs/detect/asupid_class_aware/weights/best.pt' ,\n",
    "    'yolo-agn-weights': 'runs/detect/asupid_class_agnostic/weights/best.pt' ,\n",
    "    'one-shot-weights': '../Datasets/asupid/One-shot/best_model_500_2_v1.pth',\n",
    "    'slice_sz': 1024,\n",
    "    'num-classes':44},\n",
    "    \n",
    "    'dpid': {\n",
    "    'dir': '../Datasets/DigitizePID/results/',\n",
    "    'classes-dir': '../Datasets/DigitizePID/classes/',\n",
    "    'yolo-awr-weights': 'runs/detect/dpid_class_aware/weights/best.pt' ,\n",
    "    'yolo-agn-weights': 'runs/detect/dpid_class_agnostic/weights/best.pt' ,\n",
    "    'one-shot-weights': '../Datasets/DigitizePID/One-shot/best_model_500_2_v2.pth',\n",
    "    'slice_sz': 1024,\n",
    "    'num-classes':32},\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0cefbe3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_name = 'dpid'\n",
    "dataset_dir = my_dict[f'{data_name}']['dir']\n",
    "classes_dir = my_dict[f'{data_name}']['classes-dir']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b83376bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAHI not running\n",
      "yolo to xyxy not running\n",
      "converting 100 files to xyxy format\n",
      "******* DONE yolo2xyxy*******\n",
      "Average precision values per class:\n",
      "\n",
      "Results: class 0, Recall 0.9984906926043937, Precision: 0.9981559094719196, Average_precision 0.9979614332275138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mgupta70.ASURITE\\AppData\\Local\\Temp\\ipykernel_40672\\1411018968.py:87: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, row], ignore_index=True)\n",
      "C:\\Users\\mgupta70.ASURITE\\.conda\\envs\\timeseries\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mgupta70.ASURITE\\.conda\\envs\\timeseries\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../Datasets/DigitizePID/One-shot/best_model_500_2_v2.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 52\u001b[0m\n\u001b[0;32m     49\u001b[0m weights_file \u001b[38;5;241m=\u001b[39m my_dict[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mone-shot-weights\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     50\u001b[0m num_classes \u001b[38;5;241m=\u001b[39m  my_dict[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum-classes\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m---> 52\u001b[0m \u001b[43mtransfer_labels\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds_agn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreds_agn2awr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# conver preds to XYXY format\u001b[39;00m\n\u001b[0;32m     55\u001b[0m preds_agn2awr_xyxy \u001b[38;5;241m=\u001b[39m dataset_dir \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/preds_class_agnostic_transfer_xyxy/\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "Cell \u001b[1;32mIn[2], line 106\u001b[0m, in \u001b[0;36mtransfer_labels\u001b[1;34m(src_dir, dest_dir, weights_file, num_classes)\u001b[0m\n\u001b[0;32m    104\u001b[0m model \u001b[38;5;241m=\u001b[39m torchvision\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mresnet18(pretrained\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    105\u001b[0m model\u001b[38;5;241m.\u001b[39mfc \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mLinear(model\u001b[38;5;241m.\u001b[39mfc\u001b[38;5;241m.\u001b[39min_features, num_classes)\n\u001b[1;32m--> 106\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweights_file\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    107\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m    109\u001b[0m txt_pths \u001b[38;5;241m=\u001b[39m natsorted(get_files(src_dir, extensions\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.txt\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[1;32m~\\.conda\\envs\\timeseries\\lib\\site-packages\\torch\\serialization.py:771\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[0;32m    768\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m    769\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 771\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[0;32m    772\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[0;32m    773\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[0;32m    774\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[0;32m    775\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[0;32m    776\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[1;32m~\\.conda\\envs\\timeseries\\lib\\site-packages\\torch\\serialization.py:270\u001b[0m, in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    268\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[0;32m    269\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[1;32m--> 270\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    271\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    272\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[1;32m~\\.conda\\envs\\timeseries\\lib\\site-packages\\torch\\serialization.py:251\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[1;32m--> 251\u001b[0m     \u001b[38;5;28msuper\u001b[39m(_open_file, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../Datasets/DigitizePID/One-shot/best_model_500_2_v2.pth'"
     ]
    }
   ],
   "source": [
    "#####################################\n",
    "### CLASS AWARE SAHI results #####\n",
    "#####################################\n",
    "GT_aware = dataset_dir + '/GT_class_aware/'\n",
    "GT_aware_xyxy = dataset_dir + '/GT_class_aware_xyxy/'\n",
    "# preds_aware = dataset_dir + '/preds_class_aware/'\n",
    "# preds_aware_xyxy = dataset_dir + '/preds_class_aware_xyxy/'\n",
    "\n",
    "# # Perform SAHI and make yolo txts\n",
    "# weights_file = my_dict[f'{data_name}']['yolo-awr-weights']\n",
    "slice_size = my_dict[f'{data_name}']['slice_sz']\n",
    "# perform_SAHI(GT_aware, preds_aware, weights_file,slice_size, suffix='aware', to_run =False)\n",
    "\n",
    "# # Convert GT yolo to xyxy\n",
    "# GT_yolo2xyxy(GT_aware, GT_aware_xyxy, to_run = False)\n",
    "\n",
    "# # convert preds yolo to xyxy\n",
    "# preds_yolo2xyxy(preds_aware, preds_aware_xyxy, to_run = False)\n",
    "\n",
    "\n",
    "# ##### Metrics ########\n",
    "# class_awr_df = get_detection_metrics(GT_aware_xyxy, preds_aware_xyxy, dataset_dir, df_name=f'{data_name}_class_aware')\n",
    "\n",
    "#####################################\n",
    "### CLASS AGNOSTIC SAHI results #####\n",
    "#####################################\n",
    "# class agnostic\n",
    "GT_agn = dataset_dir + '/GT_class_agnostic/' # src_dir\n",
    "GT_agn_xyxy  = dataset_dir +  '/GT_class_agnostic_xyxy/' # dest_dir2\n",
    "preds_agn =  dataset_dir + '/preds_class_agnostic/' # dest_dir1\n",
    "preds_agn_xyxy =  dataset_dir + '/preds_class_agnostic_xyxy/' # dest_dir3\n",
    "\n",
    "# Perform SAHI and make yolo txts\n",
    "weights_file = my_dict[f'{data_name}']['yolo-agn-weights']\n",
    "perform_SAHI(GT_agn, preds_agn, weights_file, slice_size, suffix='agnostic', to_run=False)\n",
    "\n",
    "# Convert GT yolo to xyxy\n",
    "GT_yolo2xyxy(GT_agn, GT_agn_xyxy, to_run = False)\n",
    "\n",
    "# convert preds yolo to xyxy\n",
    "preds_yolo2xyxy(preds_agn, preds_agn_xyxy, to_run = True)\n",
    "\n",
    "\n",
    "##### Metrics ########\n",
    "class_agn_df = get_detection_metrics(GT_agn_xyxy, preds_agn_xyxy, dataset_dir, df_name=f'{data_name}_class_agn')\n",
    "\n",
    "# Transfer labels\n",
    "preds_agn2awr = dataset_dir + '/preds_class_agnostic_transfer/'\n",
    "weights_file = my_dict[f'{data_name}']['one-shot-weights']\n",
    "num_classes =  my_dict[f'{data_name}']['num-classes']\n",
    "\n",
    "transfer_labels(preds_agn, preds_agn2awr, weights_file, num_classes)\n",
    "\n",
    "# conver preds to XYXY format\n",
    "preds_agn2awr_xyxy = dataset_dir + '/preds_class_agnostic_transfer_xyxy/'\n",
    "\n",
    "# convert preds yolo to xyxy\n",
    "preds_yolo2xyxy(preds_agn2awr, preds_agn2awr_xyxy, to_run = True)\n",
    "\n",
    "##### Metrics ########\n",
    "label_transfer_df = get_detection_metrics(GT_aware_xyxy, preds_agn2awr_xyxy, dataset_dir, df_name=f'{data_name}_label_transfer')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b4f3ca5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9983232729711601"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "87bd8e13",
   "metadata": {},
   "source": [
    "### <font color='red'> test set asupid </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34cc2768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_name = 'asupid'\n",
    "# dataset_dir = my_dict[f'{data_name}']['dir']\n",
    "# Test_GT_aware_xyxy = '../Datasets/asupid/results/Test_GT_class_aware_xyxy/'\n",
    "# Test_preds_class_awr_xyxy = '../Datasets/asupid/results/Test_preds_class_aware_xyxy/'\n",
    "# Test_preds_agn2awr_xyxy = '../Datasets/asupid/results/Test_preds_class_agnostic_transfer_xyxy/'\n",
    "\n",
    "# class_awr_df = get_detection_metrics(Test_GT_aware_xyxy, Test_preds_class_awr_xyxy, dataset_dir, df_name=f'Test_{data_name}_class_aware')\n",
    "# label_transfer_df = get_detection_metrics(Test_GT_aware_xyxy, Test_preds_agn2awr_xyxy, dataset_dir, df_name=f'Test_{data_name}_label_transfer')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca79171",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b71b4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408811ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0494a5fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006f5002",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "32a91d22",
   "metadata": {},
   "source": [
    "### utility functions for copying and pasting files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2822fd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# move ims in test set\n",
    "src_dir = Path(r\"C:\\Users\\mgupta70.ASURITE\\Dropbox (ASU)\\ASU\\PhD\\Courses\\Sem-6\\2 Jpaper-2\\DigitizePID\\patches\\results_GT\")\n",
    "txt_pths = txt_pths = natsorted(get_files(src_dir, extensions='.txt'))\n",
    "im_dir = '../Datasets/DigitizePID/original/'\n",
    "dest_dir = '../Datasets/DigitizePID/results/preds_class_agnostic_transfer/'\n",
    "os.makedirs(dest_dir, exist_ok=True)\n",
    "for txt_pth in txt_pths:\n",
    "    name = txt_pth.name\n",
    "    im_pth = im_dir+'/'+name.replace('.txt', '.jpg')\n",
    "    im_pth_dest = dest_dir+'/'+name.replace('.txt', '.jpg')\n",
    "    shutil.copy(im_pth, im_pth_dest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2e90c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# move txts in test set\n",
    "\n",
    "src_dir = Path(r\"C:\\Users\\mgupta70.ASURITE\\Dropbox (ASU)\\ASU\\PhD\\Courses\\Sem-6\\2 Jpaper-2\\DigitizePID\\patches\\results_GT\")\n",
    "txt_pths = txt_pths = natsorted(get_files(src_dir, extensions='.txt'))\n",
    "src_dir2 = '../Datasets/DigitizePID/original_agnostic_labels/'\n",
    "dest_dir = '../Datasets/DigitizePID/results/GT_class_agnostic'\n",
    "os.makedirs(dest_dir, exist_ok=True)\n",
    "for txt_pth in txt_pths:\n",
    "    name = txt_pth.name\n",
    "    src_txt_pth = src_dir2+'/'+name\n",
    "    dest_txt_pth = dest_dir+'/'+name\n",
    "    shutil.copy(src_txt_pth, dest_txt_pth)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac09eb51",
   "metadata": {},
   "source": [
    "### Github: HW - to develop code for multi-class\n",
    "https://gist.github.com/tarlen5/008809c3decf19313de216b9208f3734"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fda39315",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def yolo2xyxy(bboxes,H,W):\n",
    "    ''' bboxes is an array of shape (N,4) for x_c, y_c, w,h in yolo format'''\n",
    "    xmin = (bboxes[:,0] - bboxes[:,2]/2)*W\n",
    "    ymin = (bboxes[:,1] - bboxes[:,3]/2)*H\n",
    "    xmax = (bboxes[:,0] + bboxes[:,2]/2)*W\n",
    "    ymax = (bboxes[:,1] + bboxes[:,3]/2)*H\n",
    "\n",
    "    return np.stack([xmin,ymin,xmax,ymax], axis=1)\n",
    "\n",
    "def calc_iou_individual(pred_box, gt_box):\n",
    "    \"\"\"Calculate IoU of single predicted and ground truth box\n",
    "    Args:\n",
    "        pred_box (list of floats): location of predicted object as\n",
    "            [xmin, ymin, xmax, ymax]\n",
    "        gt_box (list of floats): location of ground truth object as\n",
    "            [xmin, ymin, xmax, ymax]\n",
    "    Returns:\n",
    "        float: value of the IoU for the two boxes.\n",
    "    Raises:\n",
    "        AssertionError: if the box is obviously malformed\n",
    "    \"\"\"\n",
    "    x1_t, y1_t, x2_t, y2_t = gt_box\n",
    "    x1_p, y1_p, x2_p, y2_p = pred_box\n",
    "\n",
    "    if (x1_p > x2_p) or (y1_p > y2_p):\n",
    "        raise AssertionError(\n",
    "            \"Prediction box is malformed? pred box: {}\".format(pred_box))\n",
    "    if (x1_t > x2_t) or (y1_t > y2_t):\n",
    "        raise AssertionError(\n",
    "            \"Ground Truth box is malformed? true box: {}\".format(gt_box))\n",
    "\n",
    "    if (x2_t < x1_p or x2_p < x1_t or y2_t < y1_p or y2_p < y1_t):\n",
    "        return 0.0\n",
    "\n",
    "    far_x = np.min([x2_t, x2_p])\n",
    "    near_x = np.max([x1_t, x1_p])\n",
    "    far_y = np.min([y2_t, y2_p])\n",
    "    near_y = np.max([y1_t, y1_p])\n",
    "\n",
    "    inter_area = (far_x - near_x + 1) * (far_y - near_y + 1)\n",
    "    true_box_area = (x2_t - x1_t + 1) * (y2_t - y1_t + 1)\n",
    "    pred_box_area = (x2_p - x1_p + 1) * (y2_p - y1_p + 1)\n",
    "    iou = inter_area / (true_box_area + pred_box_area - inter_area)\n",
    "    return iou\n",
    "\n",
    "\n",
    "def get_single_image_results(gt_boxes, pred_boxes, iou_thr):\n",
    "    \"\"\"Calculates number of true_pos, false_pos, false_neg from single batch of boxes.\n",
    "    Args:\n",
    "        gt_boxes (list of list of floats): list of locations of ground truth\n",
    "            objects as [xmin, ymin, xmax, ymax]\n",
    "        pred_boxes (dict): dict of dicts of 'boxes' (formatted like `gt_boxes`)\n",
    "            and 'scores'\n",
    "        iou_thr (float): value of IoU to consider as threshold for a\n",
    "            true prediction.\n",
    "    Returns:\n",
    "        dict: true positives (int), false positives (int), false negatives (int)\n",
    "    \"\"\"\n",
    "\n",
    "    all_pred_indices = range(len(pred_boxes))\n",
    "    all_gt_indices = range(len(gt_boxes))\n",
    "    if len(all_pred_indices) == 0:\n",
    "        tp = 0\n",
    "        fp = 0\n",
    "        fn = len(gt_boxes)\n",
    "        return {'true_pos': tp, 'false_pos': fp, 'false_neg': fn}\n",
    "    if len(all_gt_indices) == 0:\n",
    "        tp = 0\n",
    "        fp = len(pred_boxes)\n",
    "        fn = 0\n",
    "        return {'true_pos': tp, 'false_pos': fp, 'false_neg': fn}\n",
    "\n",
    "    gt_idx_thr = []\n",
    "    pred_idx_thr = []\n",
    "    ious = []\n",
    "    for ipb, pred_box in enumerate(pred_boxes):\n",
    "        for igb, gt_box in enumerate(gt_boxes):\n",
    "            iou = calc_iou_individual(pred_box, gt_box)\n",
    "            if iou > iou_thr:\n",
    "                gt_idx_thr.append(igb)\n",
    "                pred_idx_thr.append(ipb)\n",
    "                ious.append(iou)\n",
    "\n",
    "    args_desc = np.argsort(ious)[::-1]\n",
    "    if len(args_desc) == 0:\n",
    "        # No matches\n",
    "        tp = 0\n",
    "        fp = len(pred_boxes)\n",
    "        fn = len(gt_boxes)\n",
    "    else:\n",
    "        gt_match_idx = []\n",
    "        pred_match_idx = []\n",
    "        for idx in args_desc:\n",
    "            gt_idx = gt_idx_thr[idx]\n",
    "            pr_idx = pred_idx_thr[idx]\n",
    "            # If the boxes are unmatched, add them to matches\n",
    "            if (gt_idx not in gt_match_idx) and (pr_idx not in pred_match_idx):\n",
    "                gt_match_idx.append(gt_idx)\n",
    "                pred_match_idx.append(pr_idx)\n",
    "        tp = len(gt_match_idx)\n",
    "        fp = len(pred_boxes) - len(pred_match_idx)\n",
    "        fn = len(gt_boxes) - len(gt_match_idx)\n",
    "\n",
    "    return {'true_pos': tp, 'false_pos': fp, 'false_neg': fn}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09238594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class agnostic\n",
    "src_dir = '../Datasets/DigitizePID/results/test_GT_class_agnostic/'\n",
    "dest_dir = '../Datasets/DigitizePID/results/test_preds_class_agnostic/'\n",
    "weights_file = 'runs/detect/dpid_class_agnostic/weights/best.pt'\n",
    "target = parse_GT_txts(src_dir)\n",
    "preds = parse_preds_txts(dest_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bdc4f9d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'true_pos': 123, 'false_pos': 0, 'false_neg': 0}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt_boxes_cxcy, pred_boxes_cxcy, iou_thr = target[2]['boxes'], preds[2]['boxes'], 0.5\n",
    "gt_boxes = yolo2xyxy(np.array(gt_boxes_cxcy), 4561,7168).tolist()\n",
    "pred_boxes = yolo2xyxy(np.array(pred_boxes_cxcy), 4561,7168).tolist()\n",
    "get_single_image_results(gt_boxes, pred_boxes, iou_thr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84de6689",
   "metadata": {},
   "source": [
    "# Useless: torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1606b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_GT_txts(src_dir):\n",
    "    txt_pths = natsorted(get_files(src_dir, extensions='.txt'))\n",
    "    target = []\n",
    "    for txt_pth in txt_pths:\n",
    "        my_dict = {}\n",
    "        with open(txt_pth, 'r') as f:\n",
    "            name = txt_pth.name\n",
    "            print('GT parsing: ',name)\n",
    "            lines = f.readlines()\n",
    "            f.close()\n",
    "        ls = [list(map(float,l.split())) for l in lines] # each line as a list of floats\n",
    "        arr = np.stack(ls) # convert to array for easy indexing\n",
    "        class_labels = arr[:,0]\n",
    "        bboxes = arr[:,1:]\n",
    "        my_dict['boxes'] = torch.from_numpy(bboxes)\n",
    "        my_dict['labels'] = torch.from_numpy(class_labels).long()\n",
    "        target.append(my_dict)\n",
    "    return target\n",
    "\n",
    "def parse_preds_txts(dest_dir):\n",
    "    txt_pths = natsorted(get_files(dest_dir, extensions='.txt'))\n",
    "    preds = []\n",
    "    for txt_pth in txt_pths:\n",
    "        my_dict = {}\n",
    "        with open(txt_pth, 'r') as f:\n",
    "            name = txt_pth.name\n",
    "            print('Preds parsing: ',name)\n",
    "            lines = f.readlines()\n",
    "            f.close()\n",
    "        ls = [list(map(float,l.split())) for l in lines] # each line as a list of floats\n",
    "        arr = np.stack(ls) # convert to array for easy indexing\n",
    "        class_labels = arr[:,0]\n",
    "        scores = arr[:,1];#ones_array = np.ones_like(scores)\n",
    "        bboxes = arr[:,2:]\n",
    "        my_dict['boxes'] = torch.from_numpy(bboxes)\n",
    "        my_dict['scores'] = torch.from_numpy(scores)\n",
    "        my_dict['labels'] = torch.from_numpy(class_labels).long()\n",
    "        preds.append(my_dict)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5d8c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "target = parse_GT_txts(src_dir)\n",
    "preds = parse_preds_txts(dest_dir)\n",
    "metric = MeanAveragePrecision(box_format='cxcywh', iou_type='bbox', max_detection_thresholds=[1,10,100])\n",
    "metric.update(preds, target) \n",
    "print(metric.compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f339b66b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1528"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_dir = '../Datasets/asupid/results/GT_class_aware/'\n",
    "txt_pths = natsorted(get_files(src_dir, extensions='.txt'))\n",
    "counter=0\n",
    "for txt_pth in txt_pths:\n",
    "\n",
    "    with open(txt_pth, 'r') as f:\n",
    "        name = txt_pth.name\n",
    "        lines = f.readlines()\n",
    "        f.close()\n",
    "    ls = [list(map(float,l.split())) for l in lines] # each line as a list of floats\n",
    "    counter+=len(ls)\n",
    "\n",
    "\n",
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8c774ef1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84.04"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1528*5.5/100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "37f1a0a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "11930-11926"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33778be2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total test images:  7\n",
      "SAHI processing:  P09.jpg\n",
      "Performing prediction on 204 number of slices.\n",
      "SAHI processing:  P10.jpg\n",
      "Performing prediction on 204 number of slices.\n",
      "SAHI processing:  P11.jpg\n",
      "Performing prediction on 204 number of slices.\n",
      "SAHI processing:  P12.jpg\n",
      "Performing prediction on 204 number of slices.\n",
      "SAHI processing:  P13.jpg\n",
      "Performing prediction on 204 number of slices.\n",
      "SAHI processing:  P14.jpg\n",
      "Performing prediction on 204 number of slices.\n",
      "SAHI processing:  P15.jpg\n",
      "Performing prediction on 204 number of slices.\n",
      "********* DONE - SAHI *********\n"
     ]
    }
   ],
   "source": [
    "src_dir = '../Datasets/asupid2/'\n",
    "dest_dir = src_dir\n",
    "weights_file = 'runs/detect/asupid_class_aware/weights/best.pt'\n",
    "slice_size = 1024\n",
    "\n",
    "perform_SAHI(src_dir, dest_dir, weights_file,slice_size, suffix='aware', with_conf_score=False, to_run =False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39c375d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
